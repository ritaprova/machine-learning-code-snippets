#using the measurement metrics 
from sklearn import preprocessing
from sklearn.model_selection import cross_val_score,cross_val_predict
from sklearn.metrics import accuracy_score,classification_report,confusion_matrix
from sklearn.metrics import roc_auc_score

#defining a function to print the over all score 
def print_score(clf,x_train,x_test,y_train,y_test,train=True):
  '''
  v0.1 Follow the scikit learn library format in terms of input
  print the accuracy score, classification report and confusion matrix of classifier

  '''
  lb = preprocessing.LabelBinarizer()
  lb.fit(y_train)

  if train:
    '''
    training performance
    '''
    res = clf1.predict(x_train)
    print("Train result:\n")
    print("accuracy score   : {:.4f}".format(accuracy_score(y_train,res)))
    print("Classification report : \n{}\n".format(classification_report(y_train,res)))
    print("confusion matrix :\n{}\n".format(confusion_matrix(y_train,res)))
    print("ROC AUC: {0:.4f}\n".format(roc_auc_score(lb.transform(y_train),lb.transform(res))))
  
  #res = cross_val_score(clf, X_train, y_train, cv=10, scoring='accuracy')
        #print("Average Accuracy: \t {0:.4f}".format(np.mean(res)))
        #print("Accuracy SD: \t\t {0:.4f}".format(np.std(res)))
        
  elif train==False:
    '''
    test performance
    '''
    res_test = clf.predict(x_test)
    print("Test Result:\n")        
    print("accuracy score: {0:.4f}\n".format(accuracy_score(y_test,res_test)))
    print("Classification Report: \n {}\n".format(classification_report(y_test,res_test)))
    print("Confusion Matrix: \n {}\n".format(confusion_matrix(y_test,res_test)))   
    print("ROC AUC: {0:.4f}\n".format(roc_auc_score(lb.transform(y_test),lb.transform(res_test))))
